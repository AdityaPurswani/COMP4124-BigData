{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732b7bca-bc21-412c-b719-1fefdd4d40cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fit models with transposed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb75b8e3-fd47-46af-8e41-6ae54108b913",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read one hot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00477164-436e-4f31-b511-9dcd2cef545e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20111083-5d28-4db0-907a-c790fbd31aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "data_folder = '/mnt/2024-team1/'\n",
    "\n",
    "csv_data = 'JanBDRcount_transpose.csv'\n",
    "raw_path = os.path.join(data_folder, csv_data)\n",
    "raw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c498e8ea-90b0-402f-9038-55b7fbf19834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the one hot encoded file\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "one_hot_path = data_folder + \"JanBDRcount_transpose_onehot.csv\"\n",
    "\n",
    "df = spark.read.csv(one_hot_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e5c9ca0-c197-4b85-ab32-c28a165abdf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cfdb80-597b-4cf2-a865-ca7c23b5f511",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9291d02b-b2a9-4f24-85e1-34813ecfc0eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read features with p-value < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e315688-8da5-4d31-9e22-be04631c24d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_p_value_001 = spark.read.json(data_folder + \"p_value_001.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c20208-b976-423a-818d-c26d86d8da6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_p_value_001 = df_p_value_001.sort(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78bec563-b837-405c-9aac-79c7f8183c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_p_value_001_with_feat = df_p_value_001.withColumn(\"feat\", F.expr(\"substring(`0`, 1, length(`0`)-2)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b119fb6-1647-4154-a5a3-1cd905ff71c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_p_value_001_with_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b35c79-7c40-4cdb-b382-a44aed67bb39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "5e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d77eef-53b0-4b42-81c5-de143ef83803",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feat_pvalue_001 = df_p_value_001_with_feat.filter(F.col(\"1\") < 1e-4).select(\"0\").collect()\n",
    "\n",
    "feat_pvalue_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367d37e4-4d2e-4cd7-86e8-923164901a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(feat_pvalue_001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce6f0e6-8447-4c4e-97bf-3676678af5d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cast String to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fc3669-c2a4-493c-b61b-00b14310164d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(F.col(\"column\"), F.col(\"index\"), *[F.col(c).cast(IntegerType()) for c in df.columns[2:]]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71cd9e7-569b-4214-a88c-bace74aa605c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbf877b-35e1-43e0-bfee-4ab026aa914d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Combine column and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c6e2a5-f117-4d19-acfc-8d6d28dc0f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "df_combine_feat_index = df.select( \n",
    "                                F.when(F.col('column') != \"PHENOTYPE\", \n",
    "                                        F.concat(F.col('column'), F.lit(\"_\"), F.col('index')))\\\n",
    "                                        .otherwise(\"PHENOTYPE\").alias(\"feat_index\"), \"*\")\\\n",
    "                                .drop('column', 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0dfa030-9a4a-4a68-8d32-f1ac8cfbbe5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_combine_feat_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d9ccd6-cc20-4d9f-ab1e-33376851b4ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filter features with p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ff36f7b-8d31-4d97-b084-34b1712b9807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feat_li = [feat[0] for feat in feat_pvalue_001]\n",
    "feat_li = list(set(feat_li))\n",
    "feat_li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65c3202-05ba-4b4f-a186-b52cddd8e1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add label to feature list\n",
    "feat_li.append(\"PHENOTYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c681f158-b48a-4085-859e-05cac5b3a6bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_combine_feat_index.filter(F.col('feat_index').isin(feat_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7af49b-a976-4362-912f-e92a06ef790e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283c3ee1-44c0-4fc4-b6e3-6e0f6e389d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_of_feat = df_filtered.count()\n",
    "num_of_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625af458-4e04-406e-a4eb-cd8d22f86f20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transpose data\n",
    "- Two methods, one convert to Pandas before transposing, the other transpose within PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f26fe91-d300-49fa-8e1a-e8ff50ef7140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method 1:\n",
    "Convert PySpark df to Pandas to transpose\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "header_col = 'feat_index'\n",
    "\n",
    "partition_df = df_filtered.toPandas()\n",
    "\n",
    "partition_df_trans = (partition_df\n",
    "                        .set_index(header_col)\n",
    "                        .T\n",
    "                        .reset_index()\n",
    "                        .rename(columns={\"index\":header_col})\n",
    "                        .drop(header_col, axis=1))\n",
    "\n",
    "df_trans_back = spark.createDataFrame(partition_df_trans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e201b002-93eb-4f6f-b10f-7c1e80550084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_trans_back = df_trans_back.withColumn(\n",
    "    'PHENOTYPE',\n",
    "    F.when(df_trans_back.PHENOTYPE == 2, 0).otherwise(1)\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1922241a-d99c-40fc-bb17-3f1d6a42abe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a61798-01ea-436e-9a2f-7267adfc77a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Method 2:\n",
    "Transpose within PySpark using groupby and pivot\n",
    "Remove the comment of following code to run\n",
    "\"\"\"\n",
    "\n",
    "# header_col = 'feat_index'\n",
    "# cols_minus_header = df_filtered.columns\n",
    "# cols_minus_header.remove(header_col)\n",
    "\n",
    "# spark.conf.set('spark.sql.pivotMaxValues', num_of_feat)\n",
    "\n",
    "# df_temp = (df_filtered\n",
    "#        .groupBy()\n",
    "#        .pivot(header_col)\n",
    "#        .agg(F.first(F.array(cols_minus_header)))\n",
    "#        .withColumn(header_col, F.array(*map(F.lit, cols_minus_header)))\n",
    "#       )\n",
    "\n",
    "# feat_col = df_temp.columns\n",
    "# feat_col.remove(header_col)\n",
    "\n",
    "# df_trans_back = df_temp.select(F.arrays_zip(*feat_col).alias('az')).selectExpr('inline(az)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8684e88e-300a-42ac-8ec0-1a482d09ccd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_trans_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a74bf2-6807-4550-8b47-3c4373b894a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870bf709-7530-4ef9-8c50-337898835f11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e745d4-169b-4a10-9940-7b0074e1f2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check whether the data is balanced\n",
    "\n",
    "df_trans = df_trans_back.groupBy('PHENOTYPE').agg(F.count('PHENOTYPE'))\n",
    "df_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c71051-3f46-457e-83d5-375e61d62d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df_trans_back.randomSplit(weights = [0.9, 0.1], seed = 555)\n",
    "print('Train:',train_data.count())\n",
    "print('Test:',test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e20b00e-0942-4e7b-94e0-8ed06452c016",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805a4541-403b-4e1b-b96e-1b36a718d04c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cross Validation Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0621ee4-cf5b-41eb-9a3e-cc0adf32bfe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data into n pieces\n",
    "\n",
    "fold = 10\n",
    "\n",
    "data_splited_li = df_trans_back.randomSplit(weights = [1.0]*fold, seed = 555)\n",
    "len(data_splited_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaa8378-a323-4192-a54b-be2b675ccd04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_features = df_trans_back.columns\n",
    "input_features.remove(\"PHENOTYPE\")\n",
    "\n",
    "test_score_li = []\n",
    "train_score_li = []\n",
    "\n",
    "for i in range(len(data_splited_li)):\n",
    "    test_data = data_splited_li[i]\n",
    "    train_data_li = [data_splited_li[j] for j in range(len(data_splited_li)) if j!=i]\n",
    "\n",
    "    train_data = train_data_li[0]\n",
    "    for df_next in train_data_li[1:]:\n",
    "        train_data = train_data.union(df_next)\n",
    "\n",
    "    '''\n",
    "    Remove the comment of the model to use\n",
    "    '''\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=input_features, outputCol='features')\n",
    "    # rf = RandomForestClassifier(featuresCol = 'features', labelCol= 'PHENOTYPE', numTrees=100, maxDepth=5, seed=42)\n",
    "    lg = LogisticRegression(featuresCol = 'features', labelCol= 'PHENOTYPE')\n",
    "    # svm = LinearSVC(featuresCol = 'features', labelCol= 'PHENOTYPE')\n",
    "\n",
    "    model = lg.fit(assembler.transform(train_data).select('PHENOTYPE','features'))\n",
    "\n",
    "    # get testing result\n",
    "    predictions = model.transform(assembler.transform(test_data))\n",
    "\n",
    "    # get training result\n",
    "    predictions_train = model.transform(assembler.transform(train_data))\n",
    "\n",
    "    # define the evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"PHENOTYPE\", rawPredictionCol=\"prediction\")\n",
    "\n",
    "    train_score_li.append(evaluator.evaluate(predictions_train))\n",
    "    test_score_li.append(evaluator.evaluate(predictions))\n",
    "\n",
    "print(\"training score: \", train_score_li, sep='\\n')\n",
    "print(\"testing score: \", test_score_li, sep='\\n')\n",
    "\n",
    "print(\"training mean: \", np.mean(train_score_li), \" std: \", np.std(train_score_li))\n",
    "print(\"testing mean: \", np.mean(test_score_li), \" std: \", np.std(test_score_li))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6719a702-c02a-4c30-8a96-342c3d9c69ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cross Validation Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3d5d04-9f9c-4f33-bc76-51ee411285fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# set the number of partition, which is the number of model in Ensemble learning\n",
    "num_of_partition = 3\n",
    "\n",
    "\n",
    "df_cols = df_trans_back.columns\n",
    "\n",
    "def build_model(partition_iter):\n",
    "\n",
    "    partition_df = pd.DataFrame(partition_iter, columns=df_cols)\n",
    "\n",
    "    if(partition_df.shape[0] <= 0):\n",
    "        # the df is empty\n",
    "        return []\n",
    "\n",
    "    X_train = partition_df.loc[:, partition_df.columns != \"PHENOTYPE\"]\n",
    "    y_train = partition_df[\"PHENOTYPE\"]\n",
    "\n",
    "    # Change between LogisticRegression and LineraSVC model\n",
    "    rf = LogisticRegression(random_state=555)  # LinearSVC(random_state=555)\n",
    "    \n",
    "    model = rf.fit(X_train,y_train)\n",
    "\n",
    "    return [model]\n",
    "\n",
    "\n",
    "def predict(instance):\n",
    "\n",
    "    inst_features = instance[:-1]  # exclude target\n",
    "\n",
    "    # make a prediction with each model\n",
    "    predictions = [m.predict([inst_features])[0] for m in models]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def major_vote(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def transform(instance):\n",
    "    # create a new Row from the instance Row and the aggregated prediction\n",
    "    return Row(**instance.asDict(),\\\n",
    "        prediction=float(major_vote(predict(instance))))\n",
    "\n",
    "\n",
    "test_score_li = []\n",
    "train_score_li = []\n",
    "\n",
    "for i in range(len(data_splited_li)):\n",
    "    test_data = data_splited_li[i]\n",
    "    train_data_li = [data_splited_li[j] for j in range(len(data_splited_li)) if j!=i]\n",
    "\n",
    "    train_data = train_data_li[0]\n",
    "    for df_next in train_data_li[1:]:\n",
    "        train_data = train_data.union(df_next)\n",
    "\n",
    "    train_data_rdd = train_data.coalesce(num_of_partition).rdd.cache()\n",
    "    print(\"number of partition for training: \", train_data_rdd.getNumPartitions())\n",
    "\n",
    "    models = train_data_rdd.mapPartitions(build_model).collect()\n",
    "\n",
    "    # test\n",
    "    test_data_rdd = test_data.rdd.cache()\n",
    "    \n",
    "    # make testing result into form\n",
    "    pred_df = test_data_rdd.map(transform).toDF()\n",
    "\n",
    "    # get training roc\n",
    "    pred_df_train = train_data_rdd.map(transform).toDF()\n",
    "\n",
    "    # define evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=\"PHENOTYPE\", rawPredictionCol=\"prediction\")\n",
    "  \n",
    "    train_score_li.append(evaluator.evaluate(pred_df_train))\n",
    "    test_score_li.append(evaluator.evaluate(pred_df))\n",
    "\n",
    "\n",
    "print(\"training score: \", train_score_li, sep='\\n')\n",
    "print(\"testing score: \", test_score_li, sep='\\n')\n",
    "\n",
    "print(\"training mean: \", np.mean(train_score_li), \" std: \", np.std(train_score_li))\n",
    "print(\"testing mean: \", np.mean(test_score_li), \" std: \", np.std(test_score_li))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3 Model Developement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
