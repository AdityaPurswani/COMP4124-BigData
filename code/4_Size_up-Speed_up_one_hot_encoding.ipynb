{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f25fe82-3eef-4b25-9373-5f94babb5564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96692eb-7ad0-4dbe-a499-17c90f2eae14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3cd5a9-0836-4e77-bd41-e7b509a5ae52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_folder = '/mnt/2024-team1/'\n",
    "\n",
    "csv_data = 'JanBDRcount_transpose.csv'\n",
    "raw_path = os.path.join(data_folder, csv_data)\n",
    "\n",
    "raw_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d9df35-9133-4149-a59a-106347252ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(raw_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fd9cc5-587c-47f6-95da-ee020690bff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a149c4-2391-4d1f-aa31-49b163baa829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60dc93a-59eb-4cf5-8490-2a9eeb5f39cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Convert datatype to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc31c366-9191-47ed-9446-d0d712a6b584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.select(F.col(\"column\"), *[F.col(c).cast(IntegerType()) for c in df.columns[1:]])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e737fc-c368-496f-b7ef-06286337d892",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop unwanted columns\n",
    "to_remove = ['FID', 'IID', 'PAT', 'MAT']\n",
    "df_dropped = df.filter(~F.col('column').isin(to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f0b492-89b7-4c29-8731-97d1e6c8e581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a30af1-6503-47c5-8ef9-1cd1fee6a74e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fillna = df_dropped.na.fill(3)\n",
    "display(df_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2915645-6d14-41fd-a39b-d65ddcef3460",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Size-up measure decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9838ac5f-c01a-4456-a399-a47b3b2f8c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def measure_size_up(func):\n",
    "  import time\n",
    "\n",
    "  def inner(df, split_count=10, num_of_partition = None):\n",
    "    if num_of_partition == None:\n",
    "      num_of_partition = df.rdd.getNumPartitions()\n",
    "    \n",
    "    # list for record all size-up value\n",
    "    size_up_li = []\n",
    "\n",
    "    for i in range(1, split_count+1):\n",
    "      df_small = df.sample((i/split_count))\n",
    "\n",
    "      print(df_small.count())\n",
    "      print(df_small.rdd.getNumPartitions())\n",
    "      \n",
    "      # perform operations and count the run time\n",
    "      start_time = time.time()\n",
    "      func(df_small)\n",
    "\n",
    "      size_up_li.append(time.time() - start_time)\n",
    "  \n",
    "    return size_up_li\n",
    "  \n",
    "  return inner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eadce7f4-cc99-46da-a295-735ea3b1183c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Speed-up measure decorator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a124b3-3d2f-429c-8edd-8be0ce12d986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def measure_speed_up(func):\n",
    "\timport time\n",
    "\n",
    "\tdef inner(df):\n",
    "\t\tnum_of_partition = df.rdd.getNumPartitions()\n",
    "\t\tprint(\"total number of partition: \", num_of_partition)\n",
    "  \n",
    "\t\ttarget_num_of_partition = 16\n",
    "  \n",
    "\t\t# list for record all speed-up value\n",
    "\t\tspeed_up_li = []\n",
    "\n",
    "\t\tfor i in range(3, target_num_of_partition+1):\n",
    "\t\t\t\n",
    "\t\t\tif i <= num_of_partition:\n",
    "\t\t\t\tdf_small_partition = df.coalesce(i)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdf_small_partition = df.repartition(i)\n",
    "\t\t\t# check we changed the number of partition\n",
    "\t\t\tprint(df_small_partition.count())\n",
    "\t\t\tprint(df_small_partition.rdd.getNumPartitions())\n",
    "\t\t\t\n",
    "\t\t\t# perform operations and count the run time\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\tfunc(df_small_partition)\n",
    "   \n",
    "\t\t\tspeed_up_li.append(time.time() - start_time)\n",
    "\t\t\n",
    "\t\treturn speed_up_li\n",
    "  \n",
    "\treturn inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1272fa-93f8-41a5-aa2e-b3c5ffaa0e7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Measure one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea320e1-db28-44b0-975f-cfd7c3b1fc79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Size-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ca146e-c1fd-40c8-8a8d-34095297eda4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import arrays_zip, col, explode\n",
    "\n",
    "\n",
    "@measure_size_up\n",
    "def one_hot_encoding(df_fillna):\n",
    "    df_gene = df_fillna.filter(~F.col('column').isin(['SEX', 'PHENOTYPE']))\n",
    "    df_sex = df_fillna.filter('column = \"SEX\"')\n",
    "    df_target = df_fillna.filter('column = \"PHENOTYPE\"')\n",
    "\n",
    "\n",
    "    def one_hot_encode_sex(c):\n",
    "        return F.when(df_sex[c] == 1, F.lit([1, 0])).otherwise(\n",
    "            F.when(df_sex[c] == 2, F.lit([0, 1]))\n",
    "        ).alias(c)\n",
    "\n",
    "    df_sex_onehot = df_sex.select(\n",
    "    F.col(\"column\"), *list(map(one_hot_encode_sex, df.columns[1:]))\n",
    "    )\n",
    "\n",
    "    def one_hot_encode_gene(c):\n",
    "        return F.when(df_gene[c] == 0, F.lit([1, 0, 0])).otherwise(\n",
    "                F.when(df_gene[c] == 1, F.lit([0, 1, 0])).otherwise(\n",
    "                        F.when(df_gene[c] == 2, F.lit([0, 0, 1])).otherwise(F.lit([0, 0, 0]))\n",
    "                    )\n",
    "            ).alias(c)\n",
    "\n",
    "    df_gene_onehot = df_gene.select(\n",
    "    F.col(\"column\"), *list(map(one_hot_encode_gene, df.columns[1:]))\n",
    "    )\n",
    "\n",
    "    cols = df.columns[1:]\n",
    "\n",
    "    # ref: https://stackoverflow.com/questions/41027315/pyspark-split-multiple-array-columns-into-rows\n",
    "    # ref: https://stackoverflow.com/questions/69162207/pyspark-explode-list-creating-column-with-index-in-list\n",
    "\n",
    "    df_sex_exp = (df_sex_onehot\n",
    "        .withColumn(\"tmp\", arrays_zip(*cols))\n",
    "        .select(col(\"column\"), F.posexplode_outer(\"tmp\").alias(\"index\", \"tmp\"))\n",
    "        .select(col(\"column\"), col(\"index\"), *[col(f\"tmp.{c}\") for c in cols]))\n",
    "\n",
    "    df_gene_exp = (df_gene_onehot\n",
    "        .withColumn(\"tmp\", arrays_zip(*cols))\n",
    "        .select(col(\"column\"), F.posexplode_outer(\"tmp\").alias(\"index\", \"tmp\"))\n",
    "        .select(col(\"column\"), col(\"index\"), *[col(f\"tmp.{c}\") for c in cols]))\n",
    "\n",
    "\n",
    "    df_feat = df_sex_exp.union(df_gene_exp)\n",
    "\n",
    "    ## Deal with target column\n",
    "    df_target_with_index = df_target.withColumn(\"index\", F.lit(0))\n",
    "\n",
    "    # re-order columns\n",
    "    df_target_with_index = df_target_with_index.select(F.col(\"column\"), F.col(\"index\"), *[F.col(c) for c in df.columns if c not in [\"column\", \"index\"]])\n",
    "\n",
    "    ## Combine features and target columns\n",
    "    df_all = df_feat.union(df_target_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd160c3-8bdd-4f71-8379-30ac5389b8af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "size_up_li = one_hot_encoding(df_fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49eb56cb-c7f8-43f2-afc0-07300c081a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "size_up_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ffeb7b-5ff2-4fbf-92dd-73d1b30aba8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size_up_li = [224.73626255989075,\n",
    " 230.8692979812622,\n",
    " 230.81820130348206,\n",
    " 233.47459483146667,\n",
    " 222.6825487613678,\n",
    " 223.79688024520874,\n",
    " 240.78272604942322,\n",
    " 252.61592864990234,\n",
    " 234.54719829559326,\n",
    " 230.66714191436768]\n",
    "\n",
    "plt.plot([x * 0.1 for x in range(1, len(size_up_li)+1)], size_up_li)\n",
    "plt.xlabel(\"percent of data\")\n",
    "plt.ylabel(\"Run Time (sec)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b4f1a1-eb09-45b9-972f-c7e3bfeed67d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Speed-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f5961e-a356-4548-801a-d47528451102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import arrays_zip, col, explode\n",
    "\n",
    "\n",
    "@measure_speed_up\n",
    "def one_hot_encoding(df_fillna):\n",
    "    df_gene = df_fillna.filter(~F.col('column').isin(['SEX', 'PHENOTYPE']))\n",
    "    df_sex = df_fillna.filter('column = \"SEX\"')\n",
    "    df_target = df_fillna.filter('column = \"PHENOTYPE\"')\n",
    "\n",
    "\n",
    "    def one_hot_encode_sex(c):\n",
    "        return F.when(df_sex[c] == 1, F.lit([1, 0])).otherwise(\n",
    "            F.when(df_sex[c] == 2, F.lit([0, 1]))\n",
    "        ).alias(c)\n",
    "\n",
    "    df_sex_onehot = df_sex.select(\n",
    "    F.col(\"column\"), *list(map(one_hot_encode_sex, df.columns[1:]))\n",
    "    )\n",
    "\n",
    "    def one_hot_encode_gene(c):\n",
    "        return F.when(df_gene[c] == 0, F.lit([1, 0, 0])).otherwise(\n",
    "                F.when(df_gene[c] == 1, F.lit([0, 1, 0])).otherwise(\n",
    "                        F.when(df_gene[c] == 2, F.lit([0, 0, 1])).otherwise(F.lit([0, 0, 0]))\n",
    "                    )\n",
    "            ).alias(c)\n",
    "\n",
    "    df_gene_onehot = df_gene.select(\n",
    "    F.col(\"column\"), *list(map(one_hot_encode_gene, df.columns[1:]))\n",
    "    )\n",
    "\n",
    "    cols = df.columns[1:]\n",
    "\n",
    "    # ref: https://stackoverflow.com/questions/41027315/pyspark-split-multiple-array-columns-into-rows\n",
    "    # ref: https://stackoverflow.com/questions/69162207/pyspark-explode-list-creating-column-with-index-in-list\n",
    "\n",
    "    df_sex_exp = (df_sex_onehot\n",
    "        .withColumn(\"tmp\", arrays_zip(*cols))\n",
    "        .select(col(\"column\"), F.posexplode_outer(\"tmp\").alias(\"index\", \"tmp\"))\n",
    "        .select(col(\"column\"), col(\"index\"), *[col(f\"tmp.{c}\") for c in cols]))\n",
    "\n",
    "    df_gene_exp = (df_gene_onehot\n",
    "        .withColumn(\"tmp\", arrays_zip(*cols))\n",
    "        .select(col(\"column\"), F.posexplode_outer(\"tmp\").alias(\"index\", \"tmp\"))\n",
    "        .select(col(\"column\"), col(\"index\"), *[col(f\"tmp.{c}\") for c in cols]))\n",
    "\n",
    "\n",
    "    df_feat = df_sex_exp.union(df_gene_exp)\n",
    "\n",
    "    ## Deal with target column\n",
    "    df_target_with_index = df_target.withColumn(\"index\", F.lit(0))\n",
    "\n",
    "    # re-order columns\n",
    "    df_target_with_index = df_target_with_index.select(F.col(\"column\"), F.col(\"index\"), *[F.col(c) for c in df.columns if c not in [\"column\", \"index\"]])\n",
    "\n",
    "    ## Combine features and target columns\n",
    "    df_all = df_feat.union(df_target_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32facf97-2b72-4ebc-af62-60a76f23af3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "speed_up_li = one_hot_encoding(df_fillna)\n",
    "speed_up_li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acaa9d8f-f97e-4a1e-9027-45fed05b22fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "speed_up_li = [182.29031944274902,\n",
    " 170.90971517562866,\n",
    " 166.1481957435608,\n",
    " 163.8801667690277,\n",
    " 164.40883135795593,\n",
    " 163.40291213989258,\n",
    " 164.29751634597778,\n",
    " 164.64252924919128,\n",
    " 163.68030381202698,\n",
    " 168.63783502578735,\n",
    " 166.7928307056427,\n",
    " 168.07008266448975,\n",
    " 165.91625785827637,\n",
    " 164.4101107120514]\n",
    "\n",
    "plt.plot(range(3, 3 + len(speed_up_li)), speed_up_li)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Run Time (sec)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4 Size up - Speed up - one hot encoding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
