{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90c054d37f6d207b6c795082e660f097",
     "grade": false,
     "grade_id": "cell-e05969ec582a0b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Spark SQL\n",
    "## Lab assignment: Exercises with MapReduce on Spark SQL - Dataframes\n",
    "\n",
    "The aim of this notebook is to play with the DataFrame API of Apache Spark, aiming to solve the same MapReduce exercises we did for the previous labs.\n",
    "\n",
    "The key difference will be the programming style, which will be more SQL-like. You should always use the functions provided by SparkSQL, I will import these functions as 'F', so we know for sure we are using Spark functions and not Python functions!\n",
    "\n",
    "**Note**: I have decided to leave out the exercise about the list of common friends between pairs of friends with DataFrames. Whilst definitely possible, I don't see much point in doing so. You could try and let me know how you get on with it, but you might need to use User-Defined Functions, or other functions I didn't explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d8fa43bd5d45e9dd31f766244ea7a8",
     "grade": false,
     "grade_id": "cell-77734fdb789e84f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submission and marking criteria\n",
    "\n",
    "You should complete this notebook and add your solutions to it. When you are done, rename your completed notebook as `ex03.ipynb`. \n",
    "\n",
    "Important notes:\n",
    "- The **group leader** must submit the `ex03.ipynb` file on Moodle.\n",
    "- **Each member of the group** must complete the peer review survey and their contribution statement using this [link](https://forms.office.com/Pages/ResponsePage.aspx?id=7qe9Z4D970GskTWEGCkKHjZupmfSK6JKqlvGZrucaoBUM00wTlFINzdWRklMOVFNWlFVWVVCREZOTCQlQCN0PWcu). **You can only submit this survey ONCE**.\n",
    "- This lab is marked out of 100 marks. We give you 10 marks for completing exercise 0 with our help, and the three remaining exercises are worth 30 marks each.\n",
    "- The marking will be focused on:\n",
    "    - Code that does solve the task correctly (15 marks).\n",
    "    - Efficiency of the solution (15 marks).\n",
    "    - Minor mistakes will deduct marks from each exercise.\n",
    "- **Submission deadline: 11th March 2022 at 3pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9943afb7a5f822050995c1ba038ce4d",
     "grade": false,
     "grade_id": "cell-2cec9fd6229659a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cede54f3c493570a8b81ff2e2103faa4",
     "grade": false,
     "grade_id": "cell-40a6e2427a09ad72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The first thing we need to do to start working with Spark is to initialize the `SparkSession`. We will also import a few libraries we will use. *Remember if you are using Databricks that `spark` and `sc` are already available to you and don't need initializing.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Spark SQL Lab\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F  # I import all Spark Functions, like length, when, explode...\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Helper function to test the correctness of the solutions\n",
    "def test(var, val, msg=\"\"):\n",
    "    print(\"1 test passed.\") if var == val else print(\"1 test failed. \" + msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5b54d35fcd453b8d0e605376a51d483",
     "grade": false,
     "grade_id": "cell-284cbe6140883e04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 0: Word Count with DataFrames\n",
    "\n",
    "As a warm-up, let's implement the Word Count with DataFrames. As in the previous lab, you're as asked to implement a `word_count(file_path)` function that counts the number of words in a document or a number of text documents provided in the input path.\n",
    "\n",
    "**Input:** The path to a text file\n",
    "\n",
    "**Output:** (word, count) - only the 10 words with the highest frequency!\n",
    "\n",
    "Recommended steps:\n",
    "1. Read the file or files. Each line should be an element of the DataFrame.\n",
    "2. Split the lines into words. (*transformation*)\n",
    "3. Filter empty words (`''`) resulting from previous steps. (*transformation*)\n",
    "4. Count the number of occurrences of each word. (*action*)\n",
    "5. Return to the driver program the 10 most repeated words. (*action*)\n",
    "\n",
    "**Additional challenge**: Improve the word count, eliminating any punctuation marks and avoid word duplicated due to capitalisation. *Advice*: Use SQL functions like: regexp_replace, trim, col or lower.\n",
    "\n",
    "We are providing a step-by-step solution for this exercise below (yes! we are this nice!), but we suggest you try it yourself first.\n",
    "\n",
    "                                                                                                       [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5144d45aab8c1b1c7cc0c98431635141",
     "grade": false,
     "grade_id": "cell-75b6d31821a2b65a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def word_count(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "\n",
    "# Test the function with quixote.txt file\n",
    "word_count(\"data/quixote.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8a532106b3b344effb6eaebcd823835",
     "grade": false,
     "grade_id": "cell-7620a7a5daf517e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The program should pass the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f709c96a3dc96d23af1f7bc13f6890e8",
     "grade": true,
     "grade_id": "cell-ef044fd580a494e6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "top10_quixote = word_count(\"data/quixote.txt\")\n",
    "print(top10_quixote)\n",
    "test(top10_quixote, [('the', 20923), ('and', 16606), ('to', 13492), ('of', 12866), \n",
    "                                  ('that', 7164), ('a', 7003), ('in', 6860), ('I', 5756), ('he', 5640), \n",
    "                                  ('for', 4534)], \"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = spark.read.text(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to play with this for a bit, so let's cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the content of this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select(\"*\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there isn't much structure in that DataFrame. Spark has inferred a single column, as a string, and every row is a line of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, let's try to split this by blank space. To do so, we can use the `split` function from SparkSQL. This function takes two arguments: the column name where you want to apply the split, and the 'pattern' you want to use to split the string upon. This function returns an array of Column type. Similar to Column operations, this function needs to be used in conjunction with a transformation (e.g. `select`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select(F.split(\"value\", \" \")) \\\n",
    "    .show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `explode` to expand each one of the elements of the lists, so that, we create a Row for each element of the arrays in the column `value`. We will call the split function inside of the explode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "      F.explode(F.split(\"value\", \" \")) \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `alias()` to give a new name, e.g. `word`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "        F.explode(F.split(\"value\", \" \")) \\\n",
    "        .alias(\"word\") \\\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out empty words. That's very similar to what we did with RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "           F.explode(F.split(\"value\", \" \")) \\\n",
    "          .alias(\"word\")) \\\n",
    "      .filter(\"word != ''\") \\\n",
    "      .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With RDDs, we needed to transform this into a tuple k,value. But with DataFrames, we can do the same operation by telling Spark which one is the attribute that will be used to group the DataFrame (i.e. the only column: `word`).\n",
    "We can use `groupBy`, but this is a transformation that allows us to perform aggregations, it needs to be used together with an aggregation operation like count, max, avg, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we didn't get the word count, why? the method `count` as an aggregation is NOT an action! but a transformation. We still need to take it back to the driver program, by using `collect` or `show`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is not quite there yet, we wanted this to be in descending order. What we could do is to transform that aggregated DataFrame using `sort` (or `OrderBy`).  To use `sort`, we need to indicate the column in which we want to apply the operation (i.e. `count`), and we also want to do it in descending order. You could do this in different ways:\n",
    "\n",
    "`.sort(df.count.desc())`  or `sort(desc(\"count\"))`, or `sort(\"count\", ascending=False)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.select( \\\n",
    "              F.explode(F.split(\"value\", \" \")) \\\n",
    "              .alias(\"word\")) \\\n",
    "        .filter(\"word != ''\") \\\n",
    "        .groupBy(\"word\").count() \\\n",
    "        .sort(F.desc(\"count\")) \\\n",
    "        .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to unpersist your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to remove any punctuation marks, you need a function to correct all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(column):\n",
    "    return F.lower(F.trim(F.regexp_replace(column, r'[^0-9a-zA-ZñÑáéíóúÁÉÍÓÚ ]+', ''))).alias('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = spark.createDataFrame([(u'Hello!, how is it going?',),\n",
    "                                         (u' removing underscore_!',),\n",
    "                                         (u' *      Removing punctuation and blank spaces  * ,',)], ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply transform the DataFrame with that function applying the function to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.select(remove_punctuation(F.col(\"sentence\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c15f07906bb1fd48a51d1ce7427b1fa",
     "grade": false,
     "grade_id": "cell-024c26d2f51c3d48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Putting this into our word count function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9428c200908e5651b29090770e57f7e",
     "grade": true,
     "grade_id": "cell-d9afc18d9f1004e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def enhanced_word_count(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "enhanced_word_count(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e5f4cfcbfbf94108fc6b671ab7c9a3d",
     "grade": false,
     "grade_id": "cell-cd1ed4f3427ea1cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1. Histogram of word repetition\n",
    "\n",
    "Provide a histogram of word repetitions, that is, the number of words that are repeated X times:\n",
    "\n",
    "* 1 time - 3 words\n",
    "* 2 times - 10 words\n",
    "* 3 times, 20 words\n",
    "...\n",
    "\n",
    "You are asked to implement a `histogram_reps(file_path)` function in Spark that **must not** use the function `word_count(file_path)`, but it could use part of the code you did before. All the processing must be done with **DataFrames**, and there should be a single `collect()` at the end to return a list. The list must be ordered by the number of times.\n",
    "\n",
    "**Input**: The path to a text file\n",
    "\n",
    "**Output**: (number of repetitions, number of words)\n",
    "\n",
    "                                                                                                       [30 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96c253124c1be82aba643bfcb5bdac48",
     "grade": false,
     "grade_id": "cell-38476d16f40954cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def histogram_reps(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "histogram_reps('data/quixote.txt')[:20] # look at the first 20 results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a544c7eef583dce482f2602922816325",
     "grade": true,
     "grade_id": "cell-bdde40e488c56163",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hist_quixote = histogram_reps(\"data/quixote.txt\")\n",
    "test(hist_quixote[:10],[(1, 17817), (2, 5146), (3, 2291), (4, 1520), \n",
    "                                     (5, 998), (6, 737), (7, 589), (8, 439), (9, 333), (10, 288)], \"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could plot this with the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_values, y_values) = zip(*hist_quixote[:10])\n",
    "plt.bar(x_values, y_values)\n",
    "plt.title('Histogram of repetitions (up to 10)')\n",
    "plt.xlabel('Number of repetitions')\n",
    "plt.ylabel('Number of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc16f0ec55f3c445756d2dfdb8ad91a4",
     "grade": false,
     "grade_id": "cell-5f7c2d07ff5e0889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2. Histogram of the length of the words\n",
    "\n",
    "\n",
    "Provide a histogram of the length of the words. Word repetition is not a problem, so if you have the word 'bye' twice in your document, you would add 2 to the number of words of length 3.\n",
    "\n",
    "* Length 1 - 100 times\n",
    "* Length 2 - 300 times\n",
    "* Length 3 - 400 times\n",
    "...\n",
    "\n",
    "You are asked to implement a `histogram_length(file_path)` function in Spark. All the processing must be done with **DataFrames**, and there should be a single `collect()` at the end to return a list. The list must be ordered by the length of the words.\n",
    "\n",
    "**Input**: A text document or multiple text documents\n",
    "\n",
    "**Output**: (Length, number of words)\n",
    "\n",
    "**Note: We are going to assume that the maximum word length is 16 characters, so anything above that shouldn't appear in the result.**\n",
    "\n",
    "                                                                                                       [30 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efee1ed48b1a2144673c4af16a59233b",
     "grade": false,
     "grade_id": "cell-7cccb15c1a1e97fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def histogram_length(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48968bcb54f2e2fde4cf4371d8d68292",
     "grade": true,
     "grade_id": "cell-e460ec12b208f800",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "length_quixote = histogram_length(\"data/quixote.txt\")\n",
    "print(length_quixote)\n",
    "test(length_quixote, [(1, 12978), (2, 80003), (3, 98414), (4, 80717), (5, 45809), \n",
    "                                   (6, 33672), (7, 30995), (8, 19570), (9, 12350), (10, 7929),\n",
    "                                   (11, 3849), (12, 2043), (13, 963), (14, 539), (15, 251), (16, 110)],\"Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93c0dc5f92724d7291a7b99be64e2e59",
     "grade": false,
     "grade_id": "cell-4246a9f0a617488f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3. Average length of the words in a document\n",
    "\n",
    "You are now asked to implement an `average_length(file_path)` function in Spark that provides the average length of the words in a document or documents. All the processing must be done with  using **DataFrames**., and the last instruction must be the only one returning a result to the driver. \n",
    "\n",
    "**Input**: The path to a text file\n",
    "\n",
    "**Output**: Average length of the words\n",
    "\n",
    "**Note: Again, we are going to assume that the maximum word length is 16 characters, so anything above that shouldn't be used to compute the average**\n",
    "\n",
    "**Hint**: You might be able to use some built-in functions from Spark!\n",
    "\n",
    "                                                                                                       [30 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42fa941a9167c743ab567552f0327b9a",
     "grade": false,
     "grade_id": "cell-c584893574d599e2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def average_length(file_path):\n",
    "    # <FILL-IN WITH YOUR CODE>\n",
    "average_length(\"data/quixote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2274135a27bb41ba871722281a093978",
     "grade": false,
     "grade_id": "cell-d5d1b9609cbb789b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The program should pass the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b36a83b8a46e83c5c34a0467409b584",
     "grade": true,
     "grade_id": "cell-b707812456f2ad27",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "avg = average_length(\"data/quixote.txt\")\n",
    "test(round(avg,2), 4.37,'Try again!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
